{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "pythonjvsc74a57bd0e905ad718a08ed27c2c36c9ef4f1f873648da9e7705f623e5afe311501acb5e0",
   "display_name": "Python 3.8.2  ('env': venv)"
  },
  "metadata": {
   "interpreter": {
    "hash": "e905ad718a08ed27c2c36c9ef4f1f873648da9e7705f623e5afe311501acb5e0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Feature Engineering"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Extracting more information from the existing data.\n",
    "\n",
    " 1. Feature Preprocessing - Changing, updating or transforming existing features. Not creating new ones<br><br> \n",
    "\t*Continuous Variables*<br>\n",
    "\t\n",
    "\t\t- Feature Transformation\n",
    "\t\t- Feature Scaling\n",
    "\t\t- Standard Scalar\n",
    "\n",
    "\t*Categorical Variables*<br>\n",
    "\n",
    "\t\t- One Hot Encoding\n",
    "\t\t- Combine Sparse Calsses\n",
    "\n",
    "2. Feature Generation<br><br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Feature Transformation: <br>\n",
    "Replacing a variable by a function. Eg: Replace a variable by a function (log, square/cube, sqrt, reciprocal). It is very specific to the selected model <br>\n",
    "\n",
    "\t- If the relation between target and feature is non-linear. For models like LR, its good to have a liner relationship between target and features, so we can transform this variable and make relationship linear\n",
    "\t- For left or right skewed data. Normal distribution make things easier. So we might need to transform here.\n",
    "\t\t- Right Skewed -> sqrt / cuberoot (nth root) or log\n",
    "\t\t- Left Skewed -> square /cube (nth power) or exp\n",
    "\n",
    "### Feature Scaling: <br>\n",
    "For distance based algo like KNN, features should be in same scale, else we’ll get ambiguous results. Scale up or down to bring all features to same scale <br><br>\n",
    "\t- **MinMax Scalar** - Bring values between 0 and 1 <br>\n",
    "\t- **Standard Scalar** - Bring mean ~0 and standard deviation ~1 (x’ = (x- x(bar) / standard deviation )\n",
    "\n",
    "\n",
    "\n",
    "### One Hot Encoding\n",
    "### Combine Sparse Calsses\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import mean_squared_error as mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}